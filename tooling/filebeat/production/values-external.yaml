fullnameOverride: filebeat

daemonset:
  #With Kafka consumer we hit default memory limit during startup. 
  resources:
    requests:
      cpu: "100m"
      memory: "200Mi"
    limits:
      cpu: "1000m"
      memory: "400Mi"
  filebeatConfig:
    filebeat.yml: |-
      filebeat.autodiscover:
        providers:
        - type: kubernetes
          hints.enabled: true
          templates:
            - condition:
                equals:
                  kubernetes.namespace: confluent
              config:
                - type: container
                  paths:
                    - /var/log/containers/*-${data.kubernetes.container.id}.log
                  multiline.pattern: '^\[|^\{'
                  multiline.negate: true
                  multiline.match: after
                  multiline.max_lines: 3000                  
      filebeat.inputs:
      - type: kafka
        hosts:
        - kafka.confluent.svc.cluster.local:9071
        topics: ['confluent-audit-log-events']
        group_id: filebeat
        username: '${KAFKA_USERNAME}'
        password: '${KAFKA_PASSWORD}'
        ssl.enabled: true
        sasl.mechanism: plain
        ssl.certificate_authorities: ['/etc/ssl/certs/kafka-ca.pem']
        parsers:
        - ndjson:
            target: log
            addErrorKey: true
            message_key: message
            expand_keys: true
      fields:
        environment: production
        clusterType: external
      processors:
        - if:
            and:
            - regexp.message: '^{"timestamp.*'
            - has_fields: ['kubernetes.container.name']
          then:
            - dissect:
                when:
                  and:
                    - regexp.message: '^{"timestamp.*context'
                    - has_fields: ['kubernetes.container.name']
                tokenizer: '{"timestamp":"%{@timestamp}","level":"%{log.level}","context":"%{kafka.connect.context}","thread":"%{log.thread}","class":"%{log.class}","message":"%{message}"}'
                field: message
                target_prefix: ''
                overwrite_keys: true
                ignore_failure: true
            - dissect:
                when:
                  and:
                    - regexp.message: '^{"timestamp'
                    - has_fields: ['kubernetes.container.name']
                    - not:
                        has_fields: ['log.message']
                tokenizer: '{"timestamp":"%{@timestamp}","level":"%{log.level}","thread":"%{log.thread}","class":"%{log.class}","message":"%{message}"}'
                field: message
                target_prefix: ''
                overwrite_keys: true
                ignore_failure: true  
            - add_fields:
                when.has_fields: ['log.class']
                target: ''
                fields:
                  log.parsed: true
        - dissect:
            when.regexp.message: '^\d+\.\d+\.\d+\.\d+ '
            tokenizer: '%{host} %{ident} %{principal} [%{timestamp}] "%{verb} %{path} %{protocol}" %{status} %{request_size} "%{referer}" "%{user_agent}"%{tail}'
            field: message
            target_prefix: 'access'
            ignore_failure: true
        - drop_event:
            when.equals.kubernetes.container.name: filebeat
      output.logstash:
          enabled: true
          hosts: ['${LOGSTASH_HOST}']

envFrom:
  - secretRef:
      name: filebeat-kafka-credentials

extraEnvs:
  - name: LOGSTASH_HOST
    value: "x0181se.sss.se.scania.com:5061"


#Mount Kafka CA certificate  
secretMounts:
  - name: kafka-ca-certificate
    path: "/etc/ssl/certs"
    secretName: kafka-certificates
